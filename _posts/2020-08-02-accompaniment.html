---
title: Generating Musical Accompaniment with a Variational Autoencoder

description: >
  With a model trained to compress multi-instrument music into a latent codes,
  and a second model that predicts these codes given just one instrument, we can
  add drums and bass to any melody.

style: assets/pages/posts/accompaniment/style
image: /assets/pages/posts/accompaniment/accompany-teaser.png
toc: true

tags:
 - Project
 - Generative Models

published: true
code: https://github.com/maxwells-daemons/accompany-music-vae
poster: https://drive.google.com/file/d/1SYW0uhId39YXQMXmvfQQf49A6d1J4ooF/view?usp=sharing

citation: swope2020accompaniment
citation-authors: Swope, Aidan and Hollaway, Brendan and Bao, Anthony and Qin, Hongsen
---

<em>
This was the final project for
<a href="https://sites.google.com/view/cs-159-spring-2019">an undergraduate class on deep probabilistic models</a>, and was built with
<a href="https://www.linkedin.com/in/brendan-hollaway">Brendan Hollaway</a>,
<a href="https://www.linkedin.com/in/anthonybao">Anthony Bao</a>, and
<a href="https://github.com/HSQ8">Hongsen Qin</a>.
</em>

<p>
Generative machine learning models have famously been used to create new media from scratch, but an even more exciting possibility involves humans collaborating with algorithms throughout the creative process.<span class="footnote" id="footnote-1-inline"><a href="#footnote-1">&sup1;</a><span>A <a href="https://distill.pub/2017/aia/">really cool article</a> by Shan Carter and Michael Nielsen discusses in much greater depth the idea of &ldquo;artificial intelligence augmentation&rdquo; through interacting with generative models.</span></span>
While generative models are increasingly able to generate convincing images, audio, and text, human input is valuable to choose properties we want the final result to have and to incorporate parts of the human experience we haven't (yet) been able to train our models to understand.
</p>

<p>
This project explores co-composing music with a neural network that automatically generates drums and bass for a human-written melody.
You can listen to some samples from our model below:
</p>

<p class="song-title">Tetris Theme</p>
<div class="song-wrapper">
  <div class="sample-wrapper">
    <audio controls preload=auto>
      <source src="/assets/pages/posts/accompaniment/audio/tetris-accompaniment.mp3">
      Your browser does not support the audio element.
    </audio>
    <p>Generated</p>
  </div>

  <div class="sample-wrapper">
    <audio controls preload=auto>
      <source src="/assets/pages/posts/accompaniment/audio/tetris-original.mp3">
      Your browser does not support the audio element.
    </audio>
    <p>Original</p>
  </div>
</div>

<p class="song-title">Nyan Cat</p>
<div class="song-wrapper">
  <div class="sample-wrapper">
    <audio controls preload=auto>
      <source src="/assets/pages/posts/accompaniment/audio/nyan-cat-accompaniment.mp3">
      Your browser does not support the audio element.
    </audio>
    <p>Generated</p>
  </div>

  <div class="sample-wrapper">
    <audio controls preload=auto>
      <source src="/assets/pages/posts/accompaniment/audio/nyan-cat-original.mp3">
      Your browser does not support the audio element.
    </audio>
    <p>Original</p>
  </div>
</div>

<p class="song-title">In the Hall of the Mountain King</p>
<div class="song-wrapper">
  <div class="sample-wrapper">
    <audio controls preload=auto>
      <source src="/assets/pages/posts/accompaniment/audio/in-the-hall-of-the-mountain-king-accompaniment.mp3">
      Your browser does not support the audio element.
    </audio>
    <p>Generated</p>
  </div>

  <div class="sample-wrapper">
    <audio controls preload=auto>
      <source src="/assets/pages/posts/accompaniment/audio/in-the-hall-of-the-mountain-king-original.mp3">
      Your browser does not support the audio element.
    </audio>
    <p>Original</p>
  </div>
</div>

<p>
While this project uses a restricted subset of MIDI
(which is itself very restricted relative to all of what's possible with music),
and the samples therefore always sound a little elevator&#8209;music&#8209;y,
we believe that this approach would scale well to larger,
more sophisticated latent variable models, such as
<a href="">OpenAI's Jukebox</a>.
</p>

<h2 id="overview">Model Overview</h2>
<p>
Before getting into the details, here's a brief overview of how the model works at a high level.
</p>

<img src="/assets/pages/posts/accompaniment/accompany-training.svg" alt="our training and inference procedure">

<p>
The core of the model is <a href="https://magenta.tensorflow.org/music-vae">MusicVAE</a>, a pretrained model created by <a href="https://magenta.tensorflow.org/">Magenta</a>.
MusicVAE consists of an encoder, which compresses pieces of music into &ldquo;latent codes,&rdquo; and a decoder, which transforms latent codes back into music.
These latent codes are vectors of 512 floating-point numbers which are intended to capture all of the high-level properties of the music, like genre, key, time, and timings for particular events.
Both the encoder and the decoder are trained on 3-track MIDI consisting of melody, drums, and bass, with extra features like time signature changes stripped.<span class="footnote" id="footnote-2-inline"><a href="#footnote-2">&sup2;</a><span>As a result, the model doesn't work very well on music using these features.</span></span>
</p>

<p>
Because we want to generate the accompaniment given a new melody, we train a &ldquo;surrogate encoder&rdquo; to mimic the original MusicVAE encoder, while only having access to the melody.
Given a dataset of 3-track music, we use the MusicVAE encoder to produce a latent code for each song, then strip out the drums and bass and train the surrogate encoder to predict the latent code from the melody alone.
Finally, given a new melody, we use the surrogate encoder to produce a new latent code, pass that code to the MusicVAE decoder to turn the code into 3-track MIDI, and stitch the original melody back in.
</p>


<h2 id="latents-and-vaes">Latent Codes and Variational Autoencoders</h2>
TODO

<h2 id="predicting-latents">Predicting Latent Codes from a Melody</h2>
TODO

<h2 id="conclusion">Conclusion</h2>
TODO

<h2 id="Footnotes">Footnotes</h2>
<ol>
  <li>
    A <a href="https://distill.pub/2017/aia/">really cool article</a> by Shan Carter and Michael Nielsen discusses in much greater depth the idea of &ldquo;artificial intelligence augmentation&rdquo; through interacting with generative models. <a id="footnote-1" href="#footnote-1-inline">↩</a>
  </li>
  <li>
    As a result, the model doesn't work very well on music using these features. <a id="footnote-2" href="#footnote-2-inline">↩</a>
  </li>
</ol>
