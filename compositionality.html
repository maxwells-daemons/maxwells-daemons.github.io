<!DOCTYPE html>
<html lang="en-US">
  <head>
    <!-- Preconnect to resources -->
    <link rel="dns-prefetch" href="//fonts.googleapis.com">
    <link rel="dns-prefetch" href="//www.google-analytics.com">
    <link rel="dns-prefetch" href="//www.googletagmanager.com">
    <link rel="preconnect" href="//cdn.jsdelivr.net">
    <link rel="preconnect" href="//disqus.com">
    <link rel="preconnect" href="//referrer.disqus.com">
    <link rel="preconnect" href="//a.disquscdn.com">
    <link rel="preconnect" href="//c.disquscdn.com">

    
     <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-173593414-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-173593414-1');
    </script>
    

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">

    <!-- Favicon -->
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#2b5797">
    <meta name="theme-color" content="#ffffff">

    <!-- Metadata for Atom/RSS feed -->
    <link type="application/atom+xml" rel="alternate" href="https://aidanswope.com/feed.xml" title="Aidan's Research Blog" />

    <!-- jekyll-seo tag -->
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Why Neural Networks are Bad at Math | Aidan’s Research Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Why Neural Networks are Bad at Math" />
<meta name="author" content="Aidan Swope" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Compositionality is a defining feature of language, mathematics, and the real world, but neural networks struggle to understand it. This post explores what exactly compositionality is, why it’s a critical component of reasoning, and why modern learning models can’t even do arithmetic." />
<meta property="og:description" content="Compositionality is a defining feature of language, mathematics, and the real world, but neural networks struggle to understand it. This post explores what exactly compositionality is, why it’s a critical component of reasoning, and why modern learning models can’t even do arithmetic." />
<link rel="canonical" href="https://aidanswope.com/compositionality" />
<meta property="og:url" content="https://aidanswope.com/compositionality" />
<meta property="og:site_name" content="Aidan’s Research Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-07T00:00:00-07:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Aidan Swope"},"description":"Compositionality is a defining feature of language, mathematics, and the real world, but neural networks struggle to understand it. This post explores what exactly compositionality is, why it’s a critical component of reasoning, and why modern learning models can’t even do arithmetic.","url":"https://aidanswope.com/compositionality","mainEntityOfPage":{"@type":"WebPage","@id":"https://aidanswope.com/compositionality"},"@type":"BlogPosting","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://aidanswope.com/assets/icons/android-chrome-512x512.png"},"name":"Aidan Swope"},"headline":"Why Neural Networks are Bad at Math","dateModified":"2020-10-07T00:00:00-07:00","datePublished":"2020-10-07T00:00:00-07:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <!-- Link fonts and stylesheets -->
    <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,350;0,600;1,350&family=Lato:wght@400;700&display=swap" rel="stylesheet">
    
      <link rel="stylesheet" href="/assets/pages/posts/compositionality/style.css" async>
    

  <body>
    <div class="page-wrapper">
      <header>
  <nav>
    <a href="/" id="nav-home">
      Aidan's Research Blog
    </a>

    <ul>
    
      <li><a href="/about">About</a></li>
    
      <li><a href="/tags">Tags</a></li>
    
    </ul>
  </nav>
</header>

<main>
  <article>
    <h1>Why Neural Networks are Bad&nbsp;at&nbsp;Math</h1>
    <p id="writing-date">Posted on 07 October 2020</p>
    <div id="writing-summary"><p>Compositionality is a defining feature of language, mathematics, and the real world, but neural networks struggle to understand it. This post explores what exactly compositionality is, why it&rsquo;s a critical component of reasoning, and why modern learning models can&rsquo;t even do arithmetic.
</p></div>

    <ul class="tag-list-inline">
  
    <li class="tag-default">
      <a href="/tags#article">
        <!-- "tag" icon from https://heroicons.dev/ -->
        <svg fill="currentColor" viewBox="0 0 20 20" width=1em><path fill-rule="evenodd" d="M17.707 9.293a1 1 0 010 1.414l-7 7a1 1 0 01-1.414 0l-7-7A.997.997 0 012 10V5a3 3 0 013-3h5c.256 0 .512.098.707.293l7 7zM5 6a1 1 0 100-2 1 1 0 000 2z" clip-rule="evenodd"></path></svg>
         Article
      </a>
    </li>
  
    <li class="tag-default">
      <a href="/tags#generalization">
        <!-- "tag" icon from https://heroicons.dev/ -->
        <svg fill="currentColor" viewBox="0 0 20 20" width=1em><path fill-rule="evenodd" d="M17.707 9.293a1 1 0 010 1.414l-7 7a1 1 0 01-1.414 0l-7-7A.997.997 0 012 10V5a3 3 0 013-3h5c.256 0 .512.098.707.293l7 7zM5 6a1 1 0 100-2 1 1 0 000 2z" clip-rule="evenodd"></path></svg>
         Generalization
      </a>
    </li>
  
    <li class="tag-default">
      <a href="/tags#neurosymbolic-models">
        <!-- "tag" icon from https://heroicons.dev/ -->
        <svg fill="currentColor" viewBox="0 0 20 20" width=1em><path fill-rule="evenodd" d="M17.707 9.293a1 1 0 010 1.414l-7 7a1 1 0 01-1.414 0l-7-7A.997.997 0 012 10V5a3 3 0 013-3h5c.256 0 .512.098.707.293l7 7zM5 6a1 1 0 100-2 1 1 0 000 2z" clip-rule="evenodd"></path></svg>
         Neurosymbolic models
      </a>
    </li>
  


  

  

  
</ul>


    <hr>

    <div class="content-wrapper">
    
    <nav id="toc-nav">
      <h3 id="toc-header">Contents</h3>
      <ul id="table-of-contents">
  <li><a href="#compositionality">Compositionality</a>
    <ul>
      <li><a href="#linguistic-recursion">Linguistic Recursion</a></li>
      <li><a href="#formal-mathematics">Formal Mathematics</a></li>
    </ul>
  </li>
  <li><a href="#recursive-neural-networks">Recursive Neural Networks</a></li>
  <li><a href="#attention">Attention</a></li>
  <li><a href="#the-symbolic-ai-program">The Symbolic AI Program</a></li>
  <li><a href="#emergence">Emergence</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
  <li><a href="#acknowledgments">Acknowledgments</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

    </nav>
    

    <blockquote>
  <p style="margin-bottom: 0">
  The acts of the mind, wherein it exerts its power over simple ideas, are chiefly these three:
  </p>
  <ol>
    <li>Combining several simple ideas into one compound one, and thus all complex ideas are made.</li>
    <li>The second is bringing two ideas, whether simple or complex, together, and setting them by one another so as to take a view of them at once, without uniting them into one, by which it gets all its ideas of relations.</li>
    <li>The third is separating them from all other ideas that accompany them in their real existence: this is called abstraction, and thus all its general ideas are made.</li>
  </ol>
  <footer>
    John Locke, <cite>An Essay Concerning Human Understanding</cite>
  </footer>
</blockquote>

<span class="todo">TODO: Introduction</span>

<!-- <p> -->
<!-- How is it that humans can build and understand complicated things? -->
<!-- Why do senses evolved for finding berries and escaping predators, -->
<!-- and <a href="http://psychclassics.yorku.ca/Miller/">brains that can only hold about seven things at once</a>, -->
<!-- let us invent machines with thousands of parts, build subtle new mathematics upon centuries of theory, -->
<!-- and understand economies made up of millions of other thinking people? -->
<!-- </p> -->

<!-- <p> -->
<!-- The one-word answer that any programmer would give you is <strong>abstraction</strong>. -->
<!-- By thinking about a collection of things as one abstract thing and intentionally -->
<!-- forgetting the details, we can hold whole complicated systems in our heads. -->
<!-- We work with these simplified black-boxes by giving them made-up names, like -->
<!-- &ldquo;the memory allocator&rdquo; or &ldquo;the fundamental theorem of calculus,&rdquo; -->
<!-- and use them as though they were as elementary as a bit-flip or the Peano axioms. -->
<!-- </p> -->

<!-- <p> -->
<!-- Since abstraction seems so core to reasoning, and modern machine learning has shown -->
<!-- dazzling success on diverse intellectual tasks like playing board games -->
<!-- and generating natural language, it's natural to ask: to what extent do -->
<!-- neural networks form abstractions? -->
<!-- <a href="https://distill.pub/2020/circuits/zoom-in/">Research visualizing CNNs in depth</a> -->
<!-- has presented compelling evidence that these models form visual concepts by composing -->
<!-- other, simpler visual concepts &mdash; and, anyway, isn't hierarchical composition -->
<!-- what distributed representations and multi-layer neural networks are all about? -->
<!-- How do we reconcile this with the fact that -->
<!-- <a href="https://deepmind.com/research/publications/analysing-mathematical-reasoning-abilities-neural-models">state-of-the-art language models can't consistently add six integers</a>? -->
<!-- Do these visually-intuitive concepts generalize when they're combined in new ways? -->
<!-- </p> -->

<!-- <!-1- TODO: diagram from circuits -1-> -->

<!-- <p> -->
<!-- This post will argue three points: -->
<!-- <ul> -->
<!--   <li>Abstraction is a key primitive of thought, and it's possible because many problems have compositional structure.</li> -->
<!--   <li>Systematic reasoning involves recognizing and exploiting compositional structure, by intentionally forming and decomposing abstractions.</li> -->
<!--   <li>Current neural networks don't robustly generalize in domains like math because they don't compartmentalize knowledge in reusable ways.</li> -->
<!-- </ul> -->
<!-- </p> -->



<h2 id="compositionality">Compositionality</h2>

<p>
<span class="todo">TODO: Intuitive explanation</span>
</p>

<span class="todo">TODO: Interactive demo</span>

<p>
<span class="todo">TODO: Why compositionality helps us simplify problems</span>
</p>

<p>
<span class="todo">TODO: Introduce other sections</span>
</p>


<h3 id="linguistic-recursion">Linguistic Recursion</h3>

<span class="todo">TODO: Subsection</span>


<h3 id="formal-mathematics">Formal Mathematics</h3>

<span class="todo">TODO: Subsection</span>





<h2 id="recursive-neural-networks">Recursive Neural Networks</h2>

<span class="todo">TODO: Section</span>





<h2 id="attention">Attention</h2>

<span class="todo">TODO: section</span>





<h2 id="the-symbolic-ai-program">The Symbolic AI Program</h2>

<span class="todo">TODO: section</span>




<h2 id="emergence">Emergence</h2>

<p>
Finally, there&rsquo;s one important point I want to address, which I call
&ldquo;the question of emergence&rdquo;: what abilities should we explicitly build into
our learning models, and what should <em>emerge</em> naturally as the model
learns to solve problems?
</p>

<p>
For example, suppose we task a learning agent with adding integers, because
that seems like the kind of thing a genuine artificial intelligence should
be able to do.
As we&rsquo;ve seen, neural networks tend to do badly at this kind of thing.
But what if we equipped the model with a calculator? Then it would only
need to learn which buttons to press in which order. This would certainly
make the problem easier.
</p>

<p>
But we&rsquo;ve lost sight of the end goal! We don&rsquo;t need a neural network
to add integers &mdash; in fact, we provided the model with a perfectly good
calculator. Instead, integer addition is a <em>proxy</em> for reasoning, which
we study because directly working on the problem of building an artificial general
intelligence is too hard. The hope is that in solving the easier problem,
we&rsquo;ll find new techniques which will apply to the general case.
Adding such a specific external component (a calculator) means we&rsquo;re
overfitting to the proxy task at the expense of our end goal.
It would be far better to instead design models that could learn addition
on their own, because it&rsquo;s more likely that this would also let other
desirable strategies emerge &mdash; including ones we haven&rsquo;t thought of yet.
</p>

<p>
So, the question of what inductive biases we should build into our models,
and what should &ldquo;emerge&rdquo; as a higher-order consequence of learning,
is a very interesting one. It&rsquo;s clear to me that counting, arithmetic,
and the like should be emergent rather than built-in. But, given the success
of strong inductive biases like convolution over flexible supersets like
fully-connected networks, it&rsquo;s hard to claim that the model should begin
<em>tabula rasa</em>.
</p>

<p>
I don&rsquo;t know whether compositionality should be built-in or emergent.
On one hand, it seems like a critical component of many problems we care about.
On the other, consider how long it takes for people just to learn to count, add, and subtract.
It may be that a general artificial intelligence as good at dealing with
ambiguity and nuance as humans are will need to work just as hard to learn math as we do.
</p>




<h2 id="conclusion">Conclusion</h2>

<span class="todo">TODO: conclusion</span>

<h2 id="acknowledgments">Acknowledgments</h2>
<p>
I&lsquo;m grateful to my colleagues and mentors at Caltech, with whom I&lsquo;ve had
many insightful discussions, and in particular <a href="https://aypan17.github.io/">Alex Pan</a>,
<a href="https://forougha.github.io/">Forough Arabshahi</a>, and <a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a>.
<span class="todo">TODO: list people who&lsquo;ve helped edit</span>.
</p>

<p>
This post was heavily influenced by Yoshua Bengio&lsquo;s talk,
&ldquo;<a href="https://slideslive.com/38922304/from-system-1-deep-learning-to-system-2-deep-learning">From System 1 Deep Learning to System 2 Deep Learning</a>,&rdquo;
as well as Douglas Hofstadter&lsquo;s <em>Gödel, Escher, Bach: an Eternal Golden Braid</em>.
<span class="todo">TODO: finish</span>
</p>

<h2 id="footnotes">Footnotes</h2>
<ol>
</ol>


    
    

    
    </div>
  </article>

  <!-- Disqus embed code -->
  
  <div id="disqus_thread">
  <script>
  var disqus_config = function () {
    this.page.url = "https://aidanswope.com/compositionality";
    this.page.identifier = "/compositionality";
  };
  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://aidans-research-blog.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  </div>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  
</main>

      <footer id="page-footer">
  © 2020 Aidan Swope
</footer>

    </div>

  <!-- Lazy-load below-the-fold content with vanilla-lazyload -->
  <script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.1.0/dist/lazyload.min.js"></script>
  <script>
    var lazyLoadInstance = new LazyLoad({});
  </script>
  </body>
</html>
