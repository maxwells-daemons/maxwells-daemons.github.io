@misc{dhariwal2020jukebox,
      title={Jukebox: A Generative Model for Music},
      author={Prafulla Dhariwal and Heewoo Jun and Christine Payne and Jong Wook Kim and Alec Radford and Ilya Sutskever},
      year={2020},
      eprint={2005.00341},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      link={https://arxiv.org/abs/2005.00341}
}

@misc{roberts2019hierarchical,
      title={A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music},
      author={Adam Roberts and Jesse Engel and Colin Raffel and Curtis Hawthorne and Douglas Eck},
      year={2019},
      eprint={1803.05428},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      link={https://arxiv.org/abs/1803.05428}
}

@article{carter2017using,
  author = {Carter, Shan and Nielsen, Michael},
  title = {Using Artificial Intelligence to Augment Human Intelligence},
  journal = {Distill},
  year = {2017},
  note = {https://distill.pub/2017/aia},
  doi = {10.23915/distill.00009},
  link = {https://distill.pub/2017/aia/}
}

@misc{razavi2019generating,
      title={Generating Diverse High-Fidelity Images with VQ-VAE-2},
      author={Ali Razavi and Aaron van den Oord and Oriol Vinyals},
      year={2019},
      eprint={1906.00446},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      link={https://arxiv.org/abs/1906.00446}
}

@misc{OpenAI_dota,
      author = {OpenAI},
      title = {OpenAI Five},
      howpublished = {\url{https://blog.openai.com/openai-five/}},
      year = {2018},
      link = {https://blog.openai.com/openai-five/}
}

@misc{courbariaux2016binarized,
      title={Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1},
      author={Matthieu Courbariaux and Itay Hubara and Daniel Soudry and Ran El-Yaniv and Yoshua Bengio},
      year={2016},
      eprint={1602.02830},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      link = {https://arxiv.org/abs/1602.02830}
}

@misc{rastegari2016xnornet,
      title={XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks},
      author={Mohammad Rastegari and Vicente Ordonez and Joseph Redmon and Ali Farhadi},
      year={2016},
      eprint={1603.05279},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      link = {https://arxiv.org/abs/1603.05279}
}

@misc{salimans2017evolution,
      title={Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
      author={Tim Salimans and Jonathan Ho and Xi Chen and Szymon Sidor and Ilya Sutskever},
      year={2017},
      eprint={1703.03864},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      link={https://arxiv.org/abs/1703.03864}
}

@article{10.5555/2627435.2638566,
author = {Wierstra, Daan and Schaul, Tom and Glasmachers, Tobias and Sun, Yi and Peters, Jan and Schmidhuber, J\"{u}rgen},
title = {Natural Evolution Strategies},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {This paper presents Natural Evolution Strategies (NES), a recent family of black-box optimization algorithms that use the natural gradient to update a parameterized search distribution in the direction of higher expected fitness. We introduce a collection of techniques that address issues of convergence, robustness, sample complexity, computational complexity and sensitivity to hyperparameters. This paper explores a number of implementations of the NES family, such as general-purpose multi-variate normal distributions and separable distributions tailored towards search in high dimensional spaces. Experimental results show best published performance on various standard benchmarks, as well as competitive performance on others.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {949–980},
numpages = {32},
keywords = {stochastic search, natural gradient, sampling, evolution strategies, black-box optimization},
link={https://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf}
}

@article{bengio2013estimating,
  added-at = {2018-08-13T00:00:00.000+0200},
  author = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron C.},
  biburl = {https://www.bibsonomy.org/bibtex/219d5c871d077d6954593688ef0b45556/dblp},
  ee = {http://arxiv.org/abs/1308.3432},
  interhash = {e80d1f8e93f4ceb5477d2440c141d054},
  intrahash = {19d5c871d077d6954593688ef0b45556},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2018-08-14T14:10:34.000+0200},
  title = {Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation.},
  link = {http://dblp.uni-trier.de/db/journals/corr/corr1308.html#BengioLC13},
  volume = {abs/1308.3432},
  year = 2013,
}

@article{6154,
  title = {Parameter-exploring policy gradients},
  author = {Sehnke, F. and Osendorfer, C. and R{\"u}ckstiess, T. and Graves, A. and Peters, J. and Schmidhuber, J.},
  journal = {Neural Networks},
  volume = {21},
  number = {4},
  pages = {551-559},
  organization = {Max-Planck-Gesellschaft},
  school = {Biologische Kybernetik},
  month = may,
  year = {2010},
  month_numeric = {5},
  link={http://people.idsia.ch/~juergen/nn2010.pdf}
}

@article{10.1007/BF00992696,
author = {Williams, Ronald J.},
title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
year = {1992},
issue_date = {May 1992},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {8},
number = {3–4},
issn = {0885-6125},
link = {https://doi.org/10.1007/BF00992696},
doi = {10.1007/BF00992696},
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
journal = {Mach. Learn.},
month = may,
pages = {229–256},
numpages = {28},
keywords = {gradient descent, mathematical analysis, connectionist networks, Reinforcement learning}
}
